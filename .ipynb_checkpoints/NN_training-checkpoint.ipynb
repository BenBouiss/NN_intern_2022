{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d547fb-cdb8-4150-b8aa-466264d45b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3066a768-1b62-43ad-a672-20311430dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_dire(file_path):\n",
    "    if not os.path.isdir(file_path):\n",
    "        os.makedirs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383befeb-1a8a-4110-9a54-679aa0deba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datas = glob(os.getcwd() + '/Data/*.nc.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe97a86f-ab77-4ba2-a5c5-74a3bafbaa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(Datas[0])\n",
    "Clean_Data = Data.dropna()\n",
    "Clean_Data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde0327c-5a03-4bdf-b908-d62f5302313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/4\n",
      "1/4\n",
      "2/4\n",
      "3/4\n"
     ]
    }
   ],
   "source": [
    "Datas = glob(os.getcwd() + '/Data/*S.csv')\n",
    "Clean_Data = pd.DataFrame()\n",
    "for ind, d in enumerate(Datas):\n",
    "    print('{}/{}'.format(ind, len(Datas)))\n",
    "    data = pd.read_csv(d)\n",
    "    Clean_Data = pd.concat([Clean_Data, data], axis=0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43fb6d75-c86b-47d1-9162-4a4499d7a951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7483133 1870783\n"
     ]
    }
   ],
   "source": [
    "Var_names_X = ['thermalDriving', 'x', 'y', 'iceDraft', 'halineDriving']\n",
    "X = Clean_Data[Var_names_X]\n",
    "Y = Clean_Data['meltRate']\n",
    "X_train = X.sample(frac = 0.8)\n",
    "X_valid = X.drop(X_train.index)\n",
    "\n",
    "Y_train = Y.loc[X_train.index]\n",
    "Y_valid = Y.drop(X_train.index)\n",
    "print(len(X_train), len(X_valid))\n",
    "\n",
    "mean, std = X_train.mean(), X_train.std() \n",
    "meanY, stdY = Y_train.mean(), Y_train.std() \n",
    "maxY = max(Y_train)\n",
    "X_train_N, X_valid_N = np.array((X_train - mean)/std), np.array((X_valid - mean)/std)\n",
    "#Y_train_N, Y_valid_N = np.array((Y_train-meanY)/stdY), np.array((Y_valid-meanY)/stdY)\n",
    "Y_train_N, Y_valid_N = np.array(Y_train / maxY), np.array(Y_valid/maxY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e03bb7b-7709-4c12-aee8-773ae6ebe394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_init(shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape))\n",
    "    model.add(tf.keras.layers.Dense(32, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation = 'relu'))\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss = 'mse',\n",
    "                metrics = ['mae', 'mse'])\n",
    "    return model\n",
    "def Model_init2(shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape))\n",
    "    model.add(tf.keras.layers.Dense(32, activation = 'swish'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'swish'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'swish'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation = 'swish'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss = 'mse',\n",
    "                metrics = ['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba867250-1032-4f07-83a4-43003c703ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/bouissob/Script/Data/data_Ocean1_COM_NEMO-CNRS.csv',\n",
       " '/home/bouissob/Script/Data/data_Ocean4_COM_NEMO-CNRS.csv',\n",
       " '/home/bouissob/Script/Data/data_Ocean3_COM_NEMO-CNRS.csv',\n",
       " '/home/bouissob/Script/Data/data_Ocean2_COM_NEMO-CNRS.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9db763-ba68-4916-8213-04be68ad2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Datas) ==1:\n",
    "    Temp = Datas[0].split('/')\n",
    "    Name = Temp[len(Temp) - 1]\n",
    "    Name = Name.replace('data_', '')\n",
    "    Ocean_scen = Name.split('_')[0]\n",
    "    Mod_nam = Name.replace(Ocean_scen + '_', '').split('.')[0]\n",
    "else:\n",
    "    Names = []\n",
    "    for n in Datas:\n",
    "        Temp = n.split('/')\n",
    "        Name = Temp[len(Temp) - 1]\n",
    "        Name = Name.replace('data_', '')\n",
    "        Ocean_scens = Name.split('_')[0]\n",
    "        Names.append(Ocean_scens)\n",
    "    Ocean_scen = '_'.join(Names)\n",
    "    Mod_nam = Name.replace(Ocean_scens + '_', '').split('.')[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4381cc-136b-428b-9597-906f85cf75b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COM_NEMO-CNRS'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mod_nam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a54051-20f9-43db-bdbf-152666c318c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ocean1_Ocean4_Ocean3_Ocean2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ocean_scen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797dd403-f87a-400b-84d7-455e060be48c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 17:37:42.731912: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-18 17:37:42.732305: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-18 17:37:42.737471: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-02-18 17:37:43.070659: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-02-18 17:37:43.093990: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2399835000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "149663/149663 [==============================] - 259s 2ms/step - loss: 3.4140e-04 - mae: 0.0061 - mse: 3.4140e-04 - val_loss: 1.6929e-04 - val_mae: 0.0039 - val_mse: 1.6929e-04\n",
      "Epoch 2/4\n",
      "149663/149663 [==============================] - 259s 2ms/step - loss: 1.5191e-04 - mae: 0.0039 - mse: 1.5191e-04 - val_loss: 1.0979e-04 - val_mae: 0.0032 - val_mse: 1.0979e-04 ETA: 1:48 - loss: 1.5721e-04  - ETA: 5s - loss: 1.5216e-04 - mae: 0.0 - ETA: 4s - loss: 1.521 - ETA: 2s - - ETA: 0s - loss: 1.5192e-04 - mae: 0.0039 - mse: 1.5192e- - ETA: 0s - loss: 1.5192e-04 - mae: 0.0039 - mse: 1.519\n",
      "Epoch 3/4\n",
      "149663/149663 [==============================] - 259s 2ms/step - loss: 1.2380e-04 - mae: 0.0035 - mse: 1.2380e-04 - val_loss: 1.1044e-04 - val_mae: 0.0030 - val_mse: 1.1044e-04\n",
      "Epoch 4/4\n",
      "149642/149663 [============================>.] - ETA: 0s - loss: 1.1298e-04 - mae: 0.0034 - mse: 1.1298e-04"
     ]
    }
   ],
   "source": [
    "Nbr_Epoch = 4\n",
    "model = Model_init2( (len(Var_names_X), ))\n",
    "Model = model.fit(X_train_N, Y_train_N,\n",
    "                   epochs = Nbr_Epoch,\n",
    "                   batch_size = 50,\n",
    "                   validation_data = (X_valid_N, Y_valid_N))\n",
    "Model_path = os.getcwd() + '/Models/' + Mod_nam + '/' + Ocean_scen + '/'\n",
    "\n",
    "Uniq = time.time()\n",
    "Name ='Ep_{}_Input_{}_{}'.format(Nbr_Epoch, len(Var_names_X), Uniq)\n",
    "filepath = Model_path + '/' + Name + '/'\n",
    "Make_dire(filepath)\n",
    "model.save(filepath + 'model.h5')\n",
    "model.evaluate(X_train, Y_train, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678a3520-b3e5-499d-9d32-6a8ea3dae212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data = Clean_Data.loc[Clean_Data.date == 1]\n",
    "X = Data[Var_names_X]\n",
    "X = np.array((X - mean)/std)\n",
    "X = np.array(X).reshape(-1, 5, )\n",
    "Y_mod = Model.model(X)\n",
    "Data['Mod_Melt'] = Y_mod.numpy().reshape(len(Y_mod, )) * maxY\n",
    "#Data['Mod_Melt'] = (Y_mod.numpy().reshape(len(Y_mod, )) * stdY) + meanY\n",
    "Dataset = Data.set_index(['y', 'x'])\n",
    "Dataset = Dataset.to_xarray()\n",
    "Dataset.Mod_Melt.plot(cmap = plt.get_cmap('viridis'))\n",
    "plt.figure()\n",
    "Dataset.meltRate.plot(cmap = plt.get_cmap('viridis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b73f4e-9fa9-4462-a59e-3da8fc0aa7dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean.to_pickle(filepath + 'MeanX.pkl')\n",
    "std.to_pickle(filepath + 'StdX.pkl')\n",
    "np.savetxt(filepath + 'MaxY.csv', np.array(maxY).reshape(1, ))\n",
    "tmx = int(Clean_Data.loc[len(Clean_Data) - 1].date)\n",
    "#STD = pd.read_pickle(pwd + '/std.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
