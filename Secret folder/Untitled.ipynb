{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dec22a2-8089-4146-a7c6-86a9ad0050f6",
   "metadata": {},
   "source": [
    "\\section{Ben}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a22523-4761-4fae-8199-64645322644b",
   "metadata": {},
   "source": [
    "```python\n",
    "def Test(Test)\n",
    "  if not Test:\n",
    "    return Test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf78943-ae6e-4327-839f-4c9f9bc57da1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ac9f894-350d-43a9-a33b-07f85e6de875",
   "metadata": {},
   "source": [
    "This is a text.  \n",
    "This is another text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5075b0-d317-453d-bc2c-6dad524e0638",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3e3cb2a-5090-4ffe-8e52-736a2f28978f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Variability inter NNs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb8020-0116-4a07-b051-750468c6e2bc",
   "metadata": {},
   "source": [
    "Neural networks are by default variables do not produce the same output from one NN trained to the next even with the same hyperparameter. A few parameters can be set in order to get reproducible NNs :\n",
    "1. Using the same training/validation datasets helps as seeing different values would lead to different optimization steps.\n",
    "2. Setting a seed number for the weight initilization  \n",
    "```python\n",
    "        tf.random.set_seed(seed)\n",
    "```\n",
    "This function allows tensorflow to initilised the same weights run after run as long as the NN neuron architecture stays the same.\n",
    "3. Tensorflow performs a shuffling before training and inbetween epochs, a certain parameter can be set to remove said shuffling, maybe *tf.random.set_seed* works on this shuffling (?). Further test is needed.\n",
    "\n",
    "Some variability is expected between NN runs, NN sometimes specializes in certain scenarios. Exemple in my report Table 8 with NN added slope and bathymetry, a very small RMSE was obtained for Ocean4. Either this NN had initialised weights that were better suited for Ocean4 or the NN saw more Ocean4 points in batchs at the start of the optimisation. The idea behind it as far as I know is that different NN have a certain probability of producing very good parameterisation once trained. Meaning that even simpler configuration could, given enough re run produce a very good NN after training if by chances the most optimal structure in the NN are predisposed in the initilisation phase. The probability of getting such NN should therefore increase when adding new variables and increasong the NN architecture complexity (added neurons, lr ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea0ca6-7a70-4ab4-bdae-a4b212d2cdf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NN hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f604e88-3ebd-44a3-bb9f-47e87038eb46",
   "metadata": {},
   "source": [
    "## Neuron architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869b84d-8cb7-4ea2-9f87-13a9ee4a90a7",
   "metadata": {},
   "source": [
    "The NN is constructed using a input layer and stacked fully connected(dense) layers, each dense layer composed of neuron performing a weighted sum of its inputs using its optimised weights. \n",
    "Though the neuron benchmark, it was found that deeper and more complex do not provide sufficient enough increase in precision to justify their high training cost. Especially simpler inputs such as (T, S, iceD) which we had.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f215f-d1e4-4ace-b75e-6df422cc9db4",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd3f37-d786-4d7d-a391-4dc667c297fb",
   "metadata": {},
   "source": [
    "The activation fucntion is what gives the possibility for the NN to learn and reproduce non linear relationships. We chose here to use a \"swish\" activation function compared to the more popular \"Relu\" function. Tests were conducted and swish NN were found to be 10-12% better than their Relu counterparts, the trending also seemed to be that this difference is supposed to widen as we increase the NN architecture complexity.  \n",
    "It is though undeniable that Relu networks should compute faster than the Swish NN, considering that a big NN (128X5) on the biggest datasets (~3-4 millions points) takes about 20 seconds (using swish), I do not think it is such a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed0a3d-5fd5-493b-bccb-eb03c1836215",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0641725c-3a3d-4ec3-a989-10b424f1abf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f8485b-8e32-4467-a77e-831df175b97b",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf64d0-c6bb-450a-8dd2-5d6f1dc30c50",
   "metadata": {},
   "source": [
    "\n",
    "Multiple learning rate scheduling method were explored:\n",
    "1. Divided by 5 every 8 epochs\n",
    "2. Divided by 2 every 10 epochs \n",
    "3. Method plateau \n",
    "\n",
    "First observation:  \n",
    "Whenever no minimum lr is introduced to method 1., since the method very rapidly descend in lr scale. At the end of the 2nd transition 16th epochs, the NN does not train anymore due to lr too low.\n",
    "\n",
    "First motivation:\n",
    "The lr scheduling was first introduced on a less advanced variable input dataset (T(z), S(z), iceD, Dist grounding)(following second method of expolation 40 T and 40 S), due to what I referred to as an \"overshoot\" pattern on the training curves, where the NN seems to bounce around (see below).\n",
    "\n",
    "The first implementation followed method 1., the produced NN divided the val mse by 2 compared to normal setups. Overall RMSE divided by 2 aswell, main benefit seen in the representation of Ocean4 notoriously hard to reproduce due to its small melt. Introducing this method does not increase training time and is therefore only a bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ef02a-e306-4208-8303-676bf7afcfe7",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcde51a-36a4-46df-a168-046e8fde3834",
   "metadata": {},
   "source": [
    "The batch size dictates how many sample points are used for every optimization steps, it is common knowledge increasing the batch size generally leads to quicker training time at the cost of precision. However some strange behaviors were observed where NNs trained using a 1024 or 2048 batch size consistently outperformed similarly trained NN using 64 and 128 batch size.\n",
    "However using 128 or 1024 alongside any form of lr scheduling does bring all NN together. Increasing the batch size seems to have a similar effect to the adding of lr schedule.\n",
    "Further goals:\n",
    "- Possible batch size schedulling (ref https://arxiv.org/abs/1711.00489v2).\n",
    "- Possible to study the generalization potential for higher size of batches.\n",
    "\n",
    "Study went as far as $(2-3)*10^{4}$ batch size during their last scheduling step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ce38c-4d34-4acb-b278-2b7004a8cbb9",
   "metadata": {},
   "source": [
    "[//]: ![batch](Secret_images/Effect_of_batch_size.png)\n",
    "<img src=\"Secret_images/Effect_of_batch_size.png\" width=\"500\" height=\"340\">\n",
    "Training curves of val_mse for different batch size usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ccee5-4d40-4918-b75d-d559d1778076",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858358e4-dafc-494e-b387-89f801110c40",
   "metadata": {},
   "source": [
    "Dropout out layers works by disabling connections between neurones, the disabling happens at every batch end using a probability given to the function. The previously disabled is then re enabled and can be disabled again at the next dropout drawing. This method is supposed to spread learning and further train networks showing overfitting symptom. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069e13f-cb55-4490-9d9f-a3981fa1ee6f",
   "metadata": {},
   "source": [
    "Multiple setup explored  \n",
    "1. Using dropout with increased learning rate(lr) (x10-100) consistently produced non converging neural networks.\n",
    "2. Using dropout with a decreased lr through any sort of lr scheduling produced worse but still converging NNs. Plateau'ed mse generally two times higher than non dropout variant.\n",
    "3. Using dropout without any kind of lr scheduling, results matched the output of the non hybrid non dropout variant in terms of mse and val_mse.  \n",
    "4. Establishing a hybrid method with training executed in two parts, first part with dropout enabled. Then after a certain number of epochs switch to second part, where dropout layers are removed and lr scheduling introduced. The first part is supposed to create a \"stable architecture\" that could be trained further in the second part with the lr scheduling. Results matched the output of the non hybrid non dropout variant in terms of mse and val_mse.  \n",
    "\n",
    "All were tested with various dropout probability from 0-50% chance of being dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375668f-599f-4d33-8020-53892cb9db1d",
   "metadata": {},
   "source": [
    "Further goals : \n",
    "- Test the generalisation potential of both standard and hybrid dropout method in general. Maybe more precisely on the composite datasets. Even if the obtained mse is higher, the produced NN might be more generalizable.\n",
    "- Possible use of both dropout and pruning method (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00d4f3-d976-4fa3-815c-20be9487975b",
   "metadata": {},
   "source": [
    "# Generalisation potential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bde290-1a06-4570-b05f-fec2a3af76c9",
   "metadata": {},
   "source": [
    "As what was discussed in the oral presentation's discussion, the poor generalization potential might be due to overfitting. However for this case, the plots presented did had quite different conditions (calving being the main one). But definetely, is the generalisation problem encountered for the composite due to overfitting (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425cfbb-e248-4aec-a58a-406efa1bd547",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Potential solutions\n",
    "\n",
    "Based on the observations that a 96X5(96-96-96-96-96) composite trained NN performed better on outside scenarios compared to a 128X5 NN. A decrease of the NN complexity might be helpful when creating a more general NN, multiple possible ways can be explored:\n",
    "1. A decreased in network complexity, through less neurons and less layers. Or keeping the large architecture and introducing either pruning or potential dropout. \n",
    "2. Maybe disabling the lr scheduling, as the \"small details\" learned by the NN might not be generalizable relationships. \n",
    "3. Potential in exploring the use of either increased batch size 4096-8192 or a batch size scheduling method. \n",
    "4. A decreased in input complexity, might be interesting to see the generalisability of a NN trained only on T, S, iceDraft with both extrapolation method explored. \n",
    "5. Might be useful to use some sort of dropout technique here either simple or hybrid method. If the composite NN did overfit, this method might re calibrate it (?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f30efc-dedf-4e60-bf20-64e64c348a65",
   "metadata": {},
   "source": [
    "First results in increasing the potential generalization :\n",
    "The simple constant pruning does not seem to increase the NN accuracy on the non trained scenarios. What can be observed however is the variability inter NN trained, where no pattern can be inferred but random NN seem to perform good.  \n",
    "\n",
    "Decreasing the NN complexity by reducing its size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142cc0f-4fee-41cb-b970-dd8f023dc3b1",
   "metadata": {},
   "source": [
    "# Interesting outlook for further testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011ec0e-55ba-485c-a6b0-87edbe62e45d",
   "metadata": {},
   "source": [
    "A desire of mine when starting up was to developpe a fully offline (/ unsupervised) NN, meaning the data would not have to be produced by a coupled ice sheet ocean model (Heavy to run). Especially when considering the speed of the NN, this could be something spectacular.  \n",
    "To do so I believe it is possible to make small steps in this direction:\n",
    "1. Instead of our only one output (melt rate) for a given point, it is possible to predict another variable such as the iceDraft at the next time increment. This would not only make the NN emulate NEMO's melt rate (same as before) outputs but also Elmer Ice's new ice topography computations. \n",
    "2. In a second time, it would be interesting to use GAN (Generative Adversarial Networks) to learn how to reproduce realistic temperature/salinity following possible scenarios flags in the training process to make different type of forcings.\n",
    "3. The produced NN and GAN could then be used fully offline to where the GAN gives the forcing temperature near the cavity, the NN produces melt rates and its new ice topography for the next time step.\n",
    "4. From there more complex relationships could be programmed to where melt water could be taken into account for the generation of the new temperature/salinity by the GAN.\n",
    "5. Say all of this was possible with the GAN's speed being somewhat similar to the NN (hypothetically), it could be expected to produce using this fully offline method, a 100 years of simulation time (~3-4 millions points) in a minute or less. It would probably be a bit more as the new ice slopes, dist (ground/front) line need to be computed at every time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07031b3f-fa6f-4d9e-9308-35dc669f2594",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "529fe4e2-34c9-42e7-af0b-29f01135bea7",
   "metadata": {},
   "source": [
    "List benchmark name of NN:\n",
    "- Prune_benchmark_ISOMIP, Goal being to study the effect of pruning of a ISOMIP+ trained NN applied to ISOMIP+. Trained for 64 epochs, plateau lr applied and 128X5 neuron architecture. All NN used different sparsity fraction, with the use of the constant sparsity function.\n",
    "- Prune_benchmark_Composite, Goal being to study the effect of pruning of a Composite trained NN applied to the 2 other non trained MISOMIP scenarios. Trained for 64 epochs, plateau lr applied and 128X5 neuron architecture. All NN used different sparsity fraction, with the use of the constant sparsity function.\n",
    "- Benchmark swish_relu (50 epochs lr method 1 (Div by 5 every 8 no min lr introduced), main goal compare swish and relu outputs and their respective training time)\n",
    "- Drop_Var_bench_128 Benchmark done to study the variables importance by not training it. Trained for 128 epochs on the 128X5 architecture using the lr plateau method.\n",
    "- Complexity_downgrade_composite Trained for 64 epochs on the Composite dataset, plateau lr applied on either 32_32_96_96 or 96X5 neuron architecture. Main goal being to compare their outputs on the 2 remaining MISOMIP datasets compared to the big 128X5 architecture to test if less complex NN might be more generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5172c-fba8-4332-b77a-0dd1d30dad67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dac9dc89-1036-4eea-942e-2780d8d899d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
